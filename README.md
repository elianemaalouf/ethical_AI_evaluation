# Qaulity & risk management of ethical AI use in human health research

## Background and Context

Everyone has a fundamental right to benefit from scientific advancement, especially in healthcare. The development of Artificial Intelligence/Machine Learning (AI/ML) has acceler-ated significantly, attracting public attention and investment. While AI promises to revolu-tionize healthcare and health research with more accurate diagnostics, personalized treat-ments, and streamlined processes, it also poses risks to fundamental rights and raises concerns about trustworthiness. Regulatory bodies like the EU are responding with frameworks such as the "AI Act," which establishes different risk levels for AI systems, categorizing medical devices using AI as high-risk but exempting scientific research. Despite increasing use of AI in health research, as evidenced by rising approvals for AI-related projects, there is a lack of specific guidance on how health research stakeholders should practically assess and monitor AI systems within their ethical obligations. This absence of clear guidance can lead to quality issues in health-AI research. However, quality and risk assessment of health-AI, on the ethical dimensions, is not a straihtforward task. We elaborate on this complexity and recommend actions to tame it during the health research project and beyond. 

## The Issue

Evaluating and reporting on ethical AI use in health research is a multidimensional process. This complexity arises from several interacting factors:
- **Multiple principles underlying ethical AI**, such as usefulness, fairness, safety, transparency, security, and privacy, each requiring specific metrics and monitoring.
- **Interdisciplinary stakeholders**, including health researchers, AI developers, end-users, ethical committees, regulators, funding agencies, and scientific publishers, who may have different perspectives and incentives. Ineffective collaboration among these stakeholders can hinder user-centric innovation and data sharing.
- **Characteristics of the AI system** (e.g., stage of development, algorithm type, inte-gration level) combined with wide-ranging clinical research questions and data con-siderations (bias, quality, consent), all of which can introduce specific ethical risks.
- **A diversity of guidelines and frameworks** for quality assessments and reporting on AI use, making it difficult for researchers to choose the most appropriate ones for their specific context. The applicability and continuous evaluation of these guidelines remain challenging. Furthermore, a cultural conflict exists between the guideline-reliant health research culture and the more risk-tolerant digital innovation culture. The lack of structured quality assessment can slow down the implementation of AI models in practice.

## Recommendations for Action

Instead of proposing new frameworks, the focus should be on operationalizing ethical AI practices during research projects and beyond through an interdisciplinary and agile process. Key recommendations include:
- **Embed ethical AI considerations in the project life cycle**: Integrate ethical assessments from the initial idea generation through design, development, evaluation, and clinical translation. This involves forming a **project advisory committee** with diverse stakeholders for shared decision-making and accountability. Design a specific ethical **AI assessment and reporting strategy** tailored to the project's unique challenges and select validated assessment and reporting frameworks whenever possible. Publish information about this strategy to benefit the wider scientific community.
- **Professionalize health-AI projects portfolio in hosting organizations**: Organizations should maintain a portfolio of health-AI projects with stage-gate evaluations aligned with the project lifecycle. Building **"ethics as a service"** capacities within organizations can provide expertise, optimize resource use, and ensure compliance. Risk and quality management offices should extend their oversight to ethical-AI as-sessments and reporting.
- **Ethical health-AI motivated funding in research and private investments**: Funding bodies and scientific publishers should encourage and incentivize projects demonstrating rigorous ethical AI assessments from the outset and enforce transpar-ency and reproducibility requirements. Support for evaluation science, operationalization efforts, and data/code sharing is crucial. Private funders also have a responsi-bility to incentivize attention to societal responsibilities in AI products they finance.
- **Provide guidance on regulation applicability & agile regulatory processes**: Establish clear oversight and accountability mechanisms for AI performance. Regulators need to increase agility in their processes by reinforcing regulatory science, building capacity, involving stakeholders, encouraging self-regulation, proactively screening new technologies, and initiating public-private partnerships for **"AI assurance laboratories"**.

## Implementation Considerations
The current transitory phase of AI regulation in Switzerland can be challenging. A proactive approach to ethical considerations is strongly advised, with a shared responsibility among all stakeholders. Main barriers to break include **inefficient communication/coordination between stakeholders, the regulatory transitory period, and a lack of clarity on who is responsible for AI system validation**. To counter these challenges, initiatives could include early stakeholder consultations in permanent forums, integrating AI education into relevant curricula, ethics committees proposing recommended documentation for ethical-AI in research protocols, organizations providing support through data scientists and quality analysts, and a shared responsibility model for system validation involving development and operational teams with ongoing monitoring and audits.

## Policy brief document
To read the full analysis, check the full [Policy Brief document](https://github.com/elianemaalouf/ethical_AI_evaluation/blob/main/ethicalHealthAI_PB_2025.pdf). 

## Suggested citation 
Maalouf E., Le Pogam M.-A., Cotofrei P. (2025). Quality & risk management of ethical AI use in human health research. Swiss Learning Health System. 


# A catalog of health-AI evaluation & reporting frameworks : existing literature in health research
This is a non-exshaustive list and is being continuously updated.  

| **Name** | **Reference** | **Description** | **Relevance**|
|:---------|:--------------|:----------------|:-------------|
|APPRAISE-AI|https://doi.org/10.1001/jamanetworkopen.2023.35377|A tool that evaluates the methodological and reporting quality of AI prediction models specifically for clinical decision support. It aims to address the lack of a quantitative assessment tool that can evaluate the robustness and readiness of AI models for clinical use, particularly when comparing models addressing the same clinical question. The tool is designed to provide a quantitative method for evaluating the quality of AI studies across six domains: clinical relevance, data quality, methodological conduct, robustness of results, reporting quality, and reproducibility.|systematic reviews, AI-based research appraisal|
|CONSORT-AI|https://doi.org/10.1038/s41591-020-1034-x|A list 14 new checklist items meant to be reported alongside the core CONSORT 2010 items. Those items address AI-specific content and aim to explicit: intended use (and users) of the AI intevention in the clinical pathway, inlucion/exclusion criteria at the levels of participants and input data, how the AI is integrated into the trial setting, version of the AI algorithm used, data acquisition methods, data quality assessments, human-AI interaction in handling input data, outputs of the AI intervention and how they contribute to decision-making, results of analysis of performance errors and their identification process, accessibility of the AI system and its code. Related to SPIRIT-AI protocol writing guidelines.|clinical trials reports, safety, effectiveness, standardized reporting|
|DECIDE-AI|https://doi.org/10.1038/s41591-022-01772-9|A checklist of 17 AI-specific reporting items and 10 generic ones focusing on early stage clinical evaluation. The items report on aspects like how patients and users were recruited and familiarized with the system, details about the AI system itself (version, algorithm type, training data characteristics, inputs, outputs), the implementation settings and workflow, defined safety outcomes and error reporting, and human factors evaluation methods and results. It also includes reporting on participant baseline characteristics, user exposure and adherence, changes to workflow, main results, subgroup analyses, modifications made to the system during the study, and human-computer agreement. It also includes reporting requirements for ethics, patient involvement, data availability, and conflicts of interest.|early live clinical evaluation, AI-based decision support systems, safety, utility, standardized reporting|
|FUTURE-AI|https://doi.org/10.1136/bmj-2024-081554|A set of 30 recommendations to consider at different stages of the AI developement stages. The recommendations are grouped under 7 categories: fairness (bias sources, data attributes, bias evaluation and corrections); universality (clinical settings, standards usage, external evaluation, local clinical validity); traceability (risk management process, technical and clinical documentation, AI input-output quality control mechanisms, periodic auditing system, usage logging system, AI governance mechanism); usability (intended use and users requirements, human-AI interaction and oversight mechanism, training materials and activities, user experience and acceptance evaluations, safety and clinical utility evaluations); robustness (sources of data variations, representative real world data, robustness evaluation against real-world variations); explainability (definition of end-users needs and requirements for explainability, explainability evaluation with end-users); general (stakeholders engagement throughout AI lifecycle, measures for data privacy and security, measures addressing identified AI risks, evaluation plan definition, regulatory compliance assessment, identification and response to application specific ethical issues,investigation and response to social issues)|AI lifecycle, fairness, universality, traceability, usability, robustness, explainability|
|justEFAB|https://doi.org/10.1145/3593013.3594096|Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness, taking into account the sociotechnical context where the models are used and the model integration in medical ethics. ML lifecycle bias identification and mitigation|bias, fairness, ethical-design|
|ML checklist|https://www.nature.com/documents/machine-learning-checklist.pdf|checklist questions to join to new submissions on works integrating AI models to assess the level of reproducibility/replicability: availability of codes, datasets, etc.|reproducibility|
|METRIC|https://doi.org/10.1038/s41746-024-01196-4|‘Along which characteristics should data quality be evaluated when employing a dataset for trustworthy AI in medicine?’: a specialised data quality framework for medical training data comprising 15 awareness dimensions, along which developers of medical ML applications should investigate the content of a dataset. The dimensions are divided into 6 clusters: measurement process, timeliness, representativeness, informativeness, consistency, data management. No data governance or management. no specific metrics provided.| data quality evaluation|
|MI-CLAIM|https://doi.org/10.1038/s41591-020-1041-y|minimum information about clinical artificial intelligence modeling.Structured in six parts: study design, separation of data into partitions for model training and model testing, optimization and final model selection, performance evaluation, model examination, reproducible pipeline|imaging|
|Model info sheet|https://doi.org/10.1016/j.patter.2023.100804|To assess potential risk of data leakage from 8 identified types. Model info sheets are the tools to document this assessment and are treated as the researchers arguments and claims on : clean train-test separation; each feature or groups of features legitimacy for use; test set distribution fitting for the scientific interest investigated (generalizability- external validity)|data leakage, reproducibility|
|PRISMA-AI|https://doi.org/10.1038/s41591-022-02139-w|(under construction)|systematic reviews, AI-based research appraisal, standardized reporting|
|PROBAST-AI|https://doi.org/10.1136/bmjopen-2020-048008|(under construction) Extension to Prediction model Risk Of Bias ASsessment Tool PROBAST 2015. Bias assessment is done on four dimensions : participants, predictors, outcome and analysis.|prediction model, bias, standardized reporting|
|QUADAS-AI|||diagnostic tests, quality assessment|
|STARD-AI|https://doi.org/10.1136/bmjopen-2020-047709|(under construction) Extension to Standards for Reporting of Diagnostic Accuracy Study 2015|diagnostic accuracy studies, preclinical dev., offline/silent/shadow validation|
|SPIRIT-AI|https://doi.org/10.1016/S2589-7500(20)30219-3|A list of 12 new items and 3 elaborations to report along side the core SPIRIT 2013 items. Those items cover: intended use in the clinical pathway, pre-existing evidence supporting the AI-intervention and the level of prior testing before the initiation of the trial, integration requirements into the trial setting, praticipant and data level elligibility criteria, algorithm version and data acquisition, human-AI interaction in data handling, AI intervention output and its contribution to decision making, analysis of performance error, accessibility of the AI intervention and its code. Related to CONSORT-AI reporting guidelines.|clinical trials protocols, safety, effectiveness, standardized reporting|
|TRIPOD-AI|https://doi.org/10.1136/bmj.q902|Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis. |preclinical dev., prediciton model (diagnosis, prognosis) performance evaluation, offline/silent/shadow validation, standardized reporting|
|TRIPOD-LLM|https://doi.org/10.1038/s41591-024-03425-5|Extension to TRIPOD-AI to further account for Large Language Models specificities in terms of training, fintuning, evaluation and tasks|prediction models based on LLM, standardized reporting|






