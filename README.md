# Qaulity & risk management of ethical AI use in human health research

## Background and Context

Everyone has a fundamental right to benefit from scientific advancement, especially in healthcare. The development of Artificial Intelligence/Machine Learning (AI/ML) has acceler-ated significantly, attracting public attention and investment. While AI promises to revolu-tionize healthcare and health research with more accurate diagnostics, personalized treat-ments, and streamlined processes, it also poses risks to fundamental rights and raises concerns about trustworthiness. Regulatory bodies like the EU are responding with frameworks such as the "AI Act," which establishes different risk levels for AI systems, categorizing medi-cal devices using AI as high-risk but exempting scientific research. Despite increasing use of AI in health research, as evidenced by rising approvals for AI-related projects, there is a lack of specific guidance on how health research stakeholders should practically assess and moni-tor AI systems within their ethical obligations. This absence of clear guidance can lead to quality issues in health-AI research.

## The Issue

Evaluating and reporting on ethical AI use in health research is a multidimensional process. This complexity arises from several interacting factors:
- **Multiple principles underlying ethical AI**, such as usefulness, fairness, safety, transparency, security, and privacy, each requiring specific metrics and monitoring.
- **Interdisciplinary stakeholders**, including health researchers, AI developers, end-users, ethical committees, regulators, funding agencies, and scientific publishers, who may have different perspectives and incentives. Ineffective collaboration among these stakeholders can hinder user-centric innovation and data sharing.
- **Characteristics of the AI system** (e.g., stage of development, algorithm type, inte-gration level) combined with wide-ranging clinical research questions and data con-siderations (bias, quality, consent), all of which can introduce specific ethical risks.
- **A diversity of guidelines and frameworks** for quality assessments and reporting on AI use, making it difficult for researchers to choose the most appropriate ones for their specific context. The applicability and continuous evaluation of these guidelines remain challenging. Furthermore, a cultural conflict exists between the guideline-reliant health research culture and the more risk-tolerant digital innovation culture. The lack of structured quality assessment can slow down the implementation of AI models in practice.

## Recommendations for Action

Instead of proposing new frameworks, the focus should be on operationalizing ethical AI practices during research projects and beyond through an interdisciplinary and agile process. Key recommendations include:
- **Embed ethical AI considerations in the project life cycle**: Integrate ethical assessments from the initial idea generation through design, development, evaluation, and clinical translation. This involves forming a **project advisory committee** with diverse stakeholders for shared decision-making and accountability. Design a specific ethical **AI assessment and reporting strategy** tailored to the project's unique challenges and select validated assessment and reporting frameworks whenever possible. Publish information about this strategy to benefit the wider scientific community.
- **Professionalize health-AI projects portfolio in hosting organizations**: Organizations should maintain a portfolio of health-AI projects with stage-gate evaluations aligned with the project lifecycle. Building **"ethics as a service"** capacities within organizations can provide expertise, optimize resource use, and ensure compliance. Risk and quality management offices should extend their oversight to ethical-AI as-sessments and reporting.
- **Ethical health-AI motivated funding in research and private investments**: Funding bodies and scientific publishers should encourage and incentivize projects demonstrating rigorous ethical AI assessments from the outset and enforce transpar-ency and reproducibility requirements. Support for evaluation science, operationalization efforts, and data/code sharing is crucial. Private funders also have a responsi-bility to incentivize attention to societal responsibilities in AI products they finance.
- **Provide guidance on regulation applicability & agile regulatory processes**: Establish clear oversight and accountability mechanisms for AI performance. Regulators need to increase agility in their processes by reinforcing regulatory science, building capacity, involving stakeholders, encouraging self-regulation, proactively screening new technologies, and initiating public-private partnerships for **"AI assurance laboratories"**.

## Implementation Considerations
The current transitory phase of AI regulation in Switzerland can be challenging. A proactive approach to ethical considerations is strongly advised, with a shared responsibility among all stakeholders. Main barriers to break include **inefficient communication/coordination between stakeholders, the regulatory transitory period, and a lack of clarity on who is responsible for AI system validation**. To counter these challenges, initiatives could include early stakeholder consultations in permanent forums, integrating AI education into relevant curricula, ethics committees proposing recommended documentation for ethical-AI in research protocols, organizations providing support through data scientists and quality analysts, and a shared responsibility model for system validation involving development and operational teams with ongoing monitoring and audits.

## Policy brief document
To read more, check the full [Policy Brief document](https://github.com/elianemaalouf/ethical_AI_evaluation/blob/main/ethicalHealthAI_PB_2025.pdf). 

## Suggested citation 
Maalouf E., Le Pogam M.-A., Cotofrei P. (2025). Quality & risk management of ethical AI use in human health research. Swiss Learning Health System. 



